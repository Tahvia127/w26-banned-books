{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6db7eeb-9875-410a-91e6-b30b87b652a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOGRAPHIC ANALYSIS FOR BOOK BANS PROJECT\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pen_america_banned_books.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 267\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m bans_df, political_df, demographics_df = load_data()\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# Run analyses\u001b[39;00m\n\u001b[32m    270\u001b[39m state_counts = analyze_state_patterns(bans_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load all necessary datasets\"\"\"\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Load PEN America data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m bans_df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mpen_america_banned_books.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Load political data\u001b[39;00m\n\u001b[32m     26\u001b[39m political_df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mdata/geographic/state_political_classification.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = TextFileReader(filepath_or_buffer, **kwds)\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28mself\u001b[39m._make_engine(f, \u001b[38;5;28mself\u001b[39m.engine)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = get_handle(\n\u001b[32m   1881\u001b[39m     f,\n\u001b[32m   1882\u001b[39m     mode,\n\u001b[32m   1883\u001b[39m     encoding=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1884\u001b[39m     compression=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1885\u001b[39m     memory_map=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mmemory_map\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1886\u001b[39m     is_text=is_text,\n\u001b[32m   1887\u001b[39m     errors=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mencoding_errors\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstrict\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1888\u001b[39m     storage_options=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1889\u001b[39m )\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m    876\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    877\u001b[39m             errors=errors,\n\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'pen_america_banned_books.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Geographic Analysis Script for Book Bans Project\n",
    "State-level patterns, political correlations, and mapping\n",
    "Author: Joseph (Geographic & Dashboard Specialist)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy import stats\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load all necessary datasets\"\"\"\n",
    "    \n",
    "    # Load PEN America data\n",
    "    bans_df = pd.read_csv('pen_america_banned_books.csv')\n",
    "    \n",
    "    # Load political data\n",
    "    political_df = pd.read_csv('data/geographic/state_political_classification.csv')\n",
    "    \n",
    "    # Load demographics\n",
    "    demographics_df = pd.read_csv('data/geographic/state_demographics.csv')\n",
    "    \n",
    "    return bans_df, political_df, demographics_df\n",
    "\n",
    "def analyze_state_patterns(bans_df):\n",
    "    \"\"\"Analyze basic state-level patterns\"\"\"\n",
    "    \n",
    "    print(\"\\n=== STATE-LEVEL ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Count bans per state\n",
    "    state_counts = bans_df['State'].value_counts()\n",
    "    \n",
    "    print(f\"Total states with bans: {len(state_counts)}\")\n",
    "    print(f\"\\nTop 10 states by number of bans:\")\n",
    "    print(state_counts.head(10))\n",
    "    \n",
    "    print(f\"\\nBottom 10 states:\")\n",
    "    print(state_counts.tail(10))\n",
    "    \n",
    "    # Save state counts\n",
    "    state_counts_df = state_counts.reset_index()\n",
    "    state_counts_df.columns = ['State', 'Total_Bans']\n",
    "    state_counts_df.to_csv('data/processed/bans_by_state.csv', index=False)\n",
    "    \n",
    "    return state_counts_df\n",
    "\n",
    "def calculate_per_capita(bans_df, demographics_df):\n",
    "    \"\"\"Calculate bans per capita\"\"\"\n",
    "    \n",
    "    print(\"\\n=== PER CAPITA ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Count bans per state\n",
    "    state_counts = bans_df['State'].value_counts().reset_index()\n",
    "    state_counts.columns = ['State', 'Total_Bans']\n",
    "    \n",
    "    # Merge with population data\n",
    "    merged = state_counts.merge(demographics_df[['State', 'Population']], on='State', how='left')\n",
    "    \n",
    "    # Calculate per capita (per 100,000 people)\n",
    "    merged['Bans_Per_100k'] = (merged['Total_Bans'] / merged['Population']) * 100000\n",
    "    merged = merged.sort_values('Bans_Per_100k', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 states by bans per 100k population:\")\n",
    "    print(merged[['State', 'Total_Bans', 'Population', 'Bans_Per_100k']].head(10))\n",
    "    \n",
    "    # Save\n",
    "    merged.to_csv('data/processed/bans_per_capita.csv', index=False)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def political_analysis(bans_df, political_df, demographics_df):\n",
    "    \"\"\"Analyze relationship between politics and bans\"\"\"\n",
    "    \n",
    "    print(\"\\n=== POLITICAL ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Count bans per state\n",
    "    state_counts = bans_df['State'].value_counts().reset_index()\n",
    "    state_counts.columns = ['State', 'Total_Bans']\n",
    "    \n",
    "    # Merge with political and population data\n",
    "    merged = state_counts.merge(political_df, on='State', how='left')\n",
    "    merged = merged.merge(demographics_df[['State', 'Population']], on='State', how='left')\n",
    "    merged['Bans_Per_100k'] = (merged['Total_Bans'] / merged['Population']) * 100000\n",
    "    \n",
    "    # Compare by political leaning\n",
    "    red_states = merged[merged['Political_Leaning'] == 'Red']\n",
    "    blue_states = merged[merged['Political_Leaning'] == 'Blue']\n",
    "    \n",
    "    print(f\"Red states: n={len(red_states)}\")\n",
    "    print(f\"  Average bans per 100k: {red_states['Bans_Per_100k'].mean():.2f}\")\n",
    "    print(f\"  Total bans: {red_states['Total_Bans'].sum()}\")\n",
    "    \n",
    "    print(f\"\\nBlue states: n={len(blue_states)}\")\n",
    "    print(f\"  Average bans per 100k: {blue_states['Bans_Per_100k'].mean():.2f}\")\n",
    "    print(f\"  Total bans: {blue_states['Total_Bans'].sum()}\")\n",
    "    \n",
    "    # Statistical test\n",
    "    t_stat, p_value = stats.ttest_ind(red_states['Bans_Per_100k'].dropna(), \n",
    "                                       blue_states['Bans_Per_100k'].dropna())\n",
    "    \n",
    "    print(f\"\\nIndependent t-test:\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"  ** Statistically significant difference **\")\n",
    "    else:\n",
    "        print(\"  No significant difference\")\n",
    "    \n",
    "    # Correlation with Trump vote %\n",
    "    if 'Trump_2020_Percent' in merged.columns:\n",
    "        correlation = merged[['Trump_2020_Percent', 'Bans_Per_100k']].corr().iloc[0, 1]\n",
    "        print(f\"\\nCorrelation with Trump vote %: {correlation:.3f}\")\n",
    "    \n",
    "    # Save results\n",
    "    with open('documentation/political_patterns.md', 'w') as f:\n",
    "        f.write(\"# Political Patterns in Book Banning\\n\\n\")\n",
    "        f.write(f\"**Red States:**\\n\")\n",
    "        f.write(f\"- Count: {len(red_states)}\\n\")\n",
    "        f.write(f\"- Avg bans per 100k: {red_states['Bans_Per_100k'].mean():.2f}\\n\")\n",
    "        f.write(f\"- Total bans: {red_states['Total_Bans'].sum()}\\n\\n\")\n",
    "        f.write(f\"**Blue States:**\\n\")\n",
    "        f.write(f\"- Count: {len(blue_states)}\\n\")\n",
    "        f.write(f\"- Avg bans per 100k: {blue_states['Bans_Per_100k'].mean():.2f}\\n\")\n",
    "        f.write(f\"- Total bans: {blue_states['Total_Bans'].sum()}\\n\\n\")\n",
    "        f.write(f\"**Statistical Test:**\\n\")\n",
    "        f.write(f\"- t-statistic: {t_stat:.4f}\\n\")\n",
    "        f.write(f\"- p-value: {p_value:.4f}\\n\")\n",
    "        if p_value < 0.05:\n",
    "            f.write(f\"- **Conclusion:** Significant difference between red and blue states\\n\")\n",
    "        else:\n",
    "            f.write(f\"- **Conclusion:** No significant difference\\n\")\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def create_choropleth_map(data_df, column, title, filename):\n",
    "    \"\"\"Create interactive choropleth map using Plotly\"\"\"\n",
    "    \n",
    "    fig = px.choropleth(\n",
    "        data_df,\n",
    "        locations='State',\n",
    "        locationmode='USA-states',\n",
    "        color=column,\n",
    "        hover_name='State',\n",
    "        hover_data={column: ':.2f'},\n",
    "        color_continuous_scale='Reds',\n",
    "        scope='usa',\n",
    "        title=title\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_font_size=18,\n",
    "        geo=dict(\n",
    "            bgcolor='rgba(0,0,0,0)',\n",
    "            lakecolor='lightblue'\n",
    "        ),\n",
    "        height=600,\n",
    "        margin=dict(l=0, r=0, t=50, b=0)\n",
    "    )\n",
    "    \n",
    "    # Save\n",
    "    fig.write_html(f'maps/{filename}.html')\n",
    "    print(f\"✓ Created: maps/{filename}.html\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_political_map(merged_df):\n",
    "    \"\"\"Create map showing bans with political overlay\"\"\"\n",
    "    \n",
    "    # Define colors for political leaning\n",
    "    color_map = {'Red': '#FF4444', 'Blue': '#4444FF', 'Purple': '#AA44AA'}\n",
    "    merged_df['Color'] = merged_df['Political_Leaning'].map(color_map)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add choropleth for political leaning\n",
    "    fig.add_trace(go.Choropleth(\n",
    "        locations=merged_df['State'],\n",
    "        locationmode='USA-states',\n",
    "        z=merged_df['Total_Bans'],\n",
    "        colorscale='Reds',\n",
    "        text=merged_df['State'],\n",
    "        hovertemplate='<b>%{text}</b><br>' +\n",
    "                      'Political Leaning: %{customdata[0]}<br>' +\n",
    "                      'Total Bans: %{z}<br>' +\n",
    "                      'Bans per 100k: %{customdata[1]:.2f}<extra></extra>',\n",
    "        customdata=np.column_stack((merged_df['Political_Leaning'], \n",
    "                                    merged_df['Bans_Per_100k'])),\n",
    "        colorbar=dict(title=\"Total Bans\")\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Book Bans by State with Political Leaning',\n",
    "        title_font_size=18,\n",
    "        geo=dict(\n",
    "            scope='usa',\n",
    "            bgcolor='rgba(0,0,0,0)',\n",
    "            lakecolor='lightblue'\n",
    "        ),\n",
    "        height=600,\n",
    "        margin=dict(l=0, r=0, t=50, b=0)\n",
    "    )\n",
    "    \n",
    "    fig.write_html('maps/political_bans_map.html')\n",
    "    print(\"✓ Created: maps/political_bans_map.html\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def regional_analysis(bans_df):\n",
    "    \"\"\"Analyze by US region\"\"\"\n",
    "    \n",
    "    print(\"\\n=== REGIONAL ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Define regions (you may need to adjust based on your state codes)\n",
    "    regions = {\n",
    "        'Northeast': ['CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA'],\n",
    "        'South': ['DE', 'FL', 'GA', 'MD', 'NC', 'SC', 'VA', 'WV', 'AL', 'KY', \n",
    "                  'MS', 'TN', 'AR', 'LA', 'OK', 'TX'],\n",
    "        'Midwest': ['IL', 'IN', 'MI', 'OH', 'WI', 'IA', 'KS', 'MN', 'MO', 'NE', \n",
    "                    'ND', 'SD'],\n",
    "        'West': ['AZ', 'CO', 'ID', 'MT', 'NV', 'NM', 'UT', 'WY', 'AK', 'CA', \n",
    "                 'HI', 'OR', 'WA']\n",
    "    }\n",
    "    \n",
    "    # Create reverse mapping\n",
    "    state_to_region = {}\n",
    "    for region, states in regions.items():\n",
    "        for state in states:\n",
    "            state_to_region[state] = region\n",
    "    \n",
    "    # Add region column\n",
    "    bans_df['Region'] = bans_df['State'].map(state_to_region)\n",
    "    \n",
    "    # Count by region\n",
    "    regional_counts = bans_df['Region'].value_counts()\n",
    "    print(\"Bans by region:\")\n",
    "    print(regional_counts)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    regional_counts.plot(kind='bar', color='steelblue')\n",
    "    plt.title('Book Bans by US Region', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Region', fontsize=12)\n",
    "    plt.ylabel('Number of Bans', fontsize=12)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('maps/regional_comparison.png', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ Created: maps/regional_comparison.png\")\n",
    "    \n",
    "    return regional_counts\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"GEOGRAPHIC ANALYSIS FOR BOOK BANS PROJECT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load data\n",
    "    bans_df, political_df, demographics_df = load_data()\n",
    "    \n",
    "    # Run analyses\n",
    "    state_counts = analyze_state_patterns(bans_df)\n",
    "    per_capita = calculate_per_capita(bans_df, demographics_df)\n",
    "    political_data = political_analysis(bans_df, political_df, demographics_df)\n",
    "    regional_counts = regional_analysis(bans_df)\n",
    "    \n",
    "    # Create maps\n",
    "    create_choropleth_map(state_counts, 'Total_Bans', \n",
    "                         'Total Book Bans by State', 'bans_by_state')\n",
    "    create_choropleth_map(per_capita, 'Bans_Per_100k', \n",
    "                         'Book Bans Per 100,000 Population', 'bans_per_capita')\n",
    "    create_political_map(political_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(\"Check the maps/ folder for visualizations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d995b-93c9-45ce-a812-5faec46707a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
